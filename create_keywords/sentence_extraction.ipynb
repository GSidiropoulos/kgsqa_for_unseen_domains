{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "import gensim\n",
    "import nltk\n",
    "# nltk.download(\"punkt\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m spacy download en_core_web_lg\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  56658\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/mid2ent.pkl\", \"rb\") as f:\n",
    "    mid2entity = pickle.load(f)\n",
    "\n",
    "# MID->DAWT wikipedia format\n",
    "with open(\"../data/fb_id2sentences_idstr_label_type.pickle\", \"rb\") as handle:\n",
    "    mid2dawt = pickle.load(handle)\n",
    "print(\"Length: \", len(mid2dawt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_annotated_file(path):\n",
    "    sbj_mid_ = list()\n",
    "    obj_mid_ = list()\n",
    "    rel = list()\n",
    "    question = list()\n",
    "    df = pd.read_csv(path, sep=\"\\t\", usecols=[0, 1, 2, 3],\n",
    "                     names=[\"sbj\", \"relation\", \"obj\", \"question\"])\n",
    "    sbj_mid_ = df[\"sbj\"].str.replace(\"www.freebase.com/m/\", \"\").to_list()\n",
    "    rel = df[\"relation\"].str.replace(\"www.freebase.com/\", \"\").to_list()\n",
    "    obj_mid_ = df[\"obj\"].str.replace(\"www.freebase.com/m/\", \"\").to_list()\n",
    "    question = df[\"question\"].to_list()\n",
    "\n",
    "    number_of_samples = 0\n",
    "    samples_with_answer_existing = []\n",
    "    samples_with_answer_existing_occ = []  # how many sentences contain the answer entity\n",
    "    for k, v in enumerate(sbj_mid_):\n",
    "        if v in mid2dawt:\n",
    "            number_of_samples += 1\n",
    "            list_ = list()\n",
    "            for sent in mid2dawt[v]:\n",
    "                # list_.append(sent[-1])\n",
    "                list_.append(sent[-1][0])\n",
    "            if obj_mid_[k] in list_:\n",
    "                samples_with_answer_existing.append(k)\n",
    "                samples_with_answer_existing_occ.append(list_.count(obj_mid_[k]))\n",
    "            else:\n",
    "                str = \"\"\n",
    "                for sent in mid2dawt[v]:\n",
    "                    # list_.append(sent[-1])\n",
    "                    #                     print(sent[:-1])\n",
    "                    str += \" \".join(sent[:-1]).lower()\n",
    "                if obj_mid_[k] in mid2entity:\n",
    "                    if len(mid2entity[obj_mid_[k]]) != 0:\n",
    "                        if mid2entity[obj_mid_[k]][0] in str:\n",
    "                            samples_with_answer_existing.append(k)\n",
    "                            samples_with_answer_existing_occ.append(list_.count(obj_mid_[k]))\n",
    "\n",
    "    print(\"number_of_samples: \", number_of_samples)\n",
    "    print(\"samples_with_answer_existing: \", len(samples_with_answer_existing))\n",
    "\n",
    "    c = 0\n",
    "    for occ in samples_with_answer_existing_occ:\n",
    "        if occ > 1:\n",
    "            c += 1\n",
    "    print(\"More than one occurences of answer: \", c)\n",
    "\n",
    "    return sbj_mid_, rel, question, obj_mid_, samples_with_answer_existing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_centroid(sent):\n",
    "    words_sum = 0\n",
    "    emb_sum = 0\n",
    "    for word in sent:\n",
    "        if word in w2v_model:\n",
    "            words_sum += 1\n",
    "            emb_sum += w2v_model.get_vector(word)\n",
    "\n",
    "    if words_sum == 0:\n",
    "        centroid = np.ones(300) * 999999\n",
    "\n",
    "    else:\n",
    "        centroid = emb_sum / words_sum\n",
    "\n",
    "    return centroid\n",
    "\n",
    "\n",
    "def get_scores(corpus, relation):\n",
    "    corpus_centroids = []\n",
    "    for sentence in corpus:\n",
    "        centr = calculate_centroid(sentence)\n",
    "        corpus_centroids.append(centr)\n",
    "\n",
    "    relation_centroid = calculate_centroid(relation)\n",
    "    scores = w2v_model.cosine_similarities(relation_centroid, corpus_centroids)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def preprocess(string):\n",
    "    tokens = nltk.word_tokenize(string)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def lowercase_sentences(corpus):\n",
    "    return [[token.lower() for token in sentence] for sentence in corpus]\n",
    "\n",
    "\n",
    "def uniquify_list(l):\n",
    "    set_of_tuples = set(tuple(row) for row in l)\n",
    "    list_of_list = [list(item) for item in set(tuple(row) for row in set_of_tuples)]\n",
    "    return list_of_list\n",
    "\n",
    "\n",
    "def sentences_remove_fbid(corpus):\n",
    "    return [[token for token in sentence[:-1]] for sentence in corpus]\n",
    "\n",
    "\n",
    "def remove_sentences_with_multiple_occurences(corpus, obj_mid_):\n",
    "    \"\"\"Removes sentences multiple occurences of a sentence. If a sentence containing the answer entity\n",
    "    appears multiple times keeps only the one with the answer. Otherwise it keeps the first that encounter.\"\"\"\n",
    "\n",
    "    corpus_new = uniquify_list(sentences_remove_fbid(corpus))\n",
    "\n",
    "    # key-> position in the new corpus value-> the freebase id that we will keep\n",
    "    indx2fb_id = dict()\n",
    "    for i in range(len(corpus_new)):\n",
    "        indx2fb_id[i] = 99\n",
    "\n",
    "    for i, sent_n_id in enumerate(corpus):\n",
    "        sent = sent_n_id[:-1]\n",
    "        fb_id = sent_n_id[-1][0]  # when using the extended version\n",
    "        id_label_type = sent_n_id[-1]\n",
    "        indx = corpus_new.index(sent)\n",
    "        if indx2fb_id[indx] == 99:\n",
    "            indx2fb_id[indx] = id_label_type  # fb_id\n",
    "        elif fb_id == obj_mid_:\n",
    "            indx2fb_id[indx] = id_label_type  # fb_id\n",
    "\n",
    "    return corpus_new, indx2fb_id\n",
    "\n",
    "\n",
    "def split_unpunctuated_text(corpus_new, indx2fb_id):\n",
    "    corpus_new_sentences = []\n",
    "    indx2fb_id_newsent = dict()\n",
    "    indx = 0\n",
    "    #     c =0\n",
    "    #     cc = 0\n",
    "    for key, value in indx2fb_id.items():\n",
    "        #         cc += 1\n",
    "        doc = nlp(\" \".join(corpus_new[key]))\n",
    "        found = False\n",
    "        for sent in doc.sents:\n",
    "            if value[1] in sent.text:\n",
    "                sent_tokenized_lowered = preprocess(sent.text.lower())\n",
    "                corpus_new_sentences.append(sent_tokenized_lowered)\n",
    "                indx2fb_id_newsent[indx] = value\n",
    "                indx += 1\n",
    "\n",
    "            else:\n",
    "                sent_tokenized_lowered = preprocess(sent.text.lower())\n",
    "                corpus_new_sentences.append(sent_tokenized_lowered)\n",
    "                indx2fb_id_newsent[indx] = [0, 0, 0]\n",
    "                indx += 1\n",
    "\n",
    "    return corpus_new_sentences, indx2fb_id_newsent\n",
    "\n",
    "\n",
    "def split_unpunctuated_text_add_placeholders(corpus_new, indx2fb_id, pattern):\n",
    "    corpus_new_sentences = []\n",
    "    indx2fb_id_newsent = dict()\n",
    "    indx = 0\n",
    "    for key, value in indx2fb_id.items():\n",
    "        doc = nlp(\" \".join(corpus_new[key]))\n",
    "        for sent in doc.sents:\n",
    "            if value[1] in sent.text:\n",
    "                text_with_placeholders = sent.text\n",
    "                for k, v in pattern.items():\n",
    "                    text_with_placeholders = re.sub(r\"\\b{}\\b\".format(re.escape(k)), v,\n",
    "                                                    text_with_placeholders)  # text_with_placeholders.replace(k,v)\n",
    "\n",
    "                sent_tokenized_lowered = preprocess(text_with_placeholders.lower())\n",
    "                corpus_new_sentences.append(sent_tokenized_lowered)\n",
    "                indx2fb_id_newsent[indx] = value\n",
    "                indx += 1\n",
    "            else:\n",
    "                text_with_placeholders = sent.text\n",
    "                for k, v in pattern.items():\n",
    "                    text_with_placeholders = re.sub(r\"\\b{}\\b\".format(re.escape(k)), v,\n",
    "                                                    text_with_placeholders)  # text_with_placeholders.replace(k,v)\n",
    "\n",
    "                sent_tokenized_lowered = preprocess(text_with_placeholders.lower())\n",
    "                corpus_new_sentences.append(sent_tokenized_lowered)\n",
    "                indx2fb_id_newsent[indx] = [0, 0, 0]\n",
    "                indx += 1\n",
    "\n",
    "    return corpus_new_sentences, indx2fb_id_newsent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_replacement(sorted_idlbtype):\n",
    "    rep = dict()\n",
    "    for idlbtype in sorted_idlbtype:\n",
    "        rep[idlbtype[1]] = idlbtype[2] + \"_\"\n",
    "    return rep\n",
    "\n",
    "\n",
    "def create_ngrams(text):\n",
    "    n_grams = list()\n",
    "    token = nltk.word_tokenize(text)  # token = text.split()\n",
    "\n",
    "    for i in range(1, len(token) + 1):\n",
    "        n_gram = ngrams(token, i)\n",
    "        for gram in n_gram:\n",
    "            n_grams.append(\" \".join(gram))\n",
    "    return n_grams\n",
    "\n",
    "\n",
    "def preprocess_rtrn_string(string):\n",
    "    tokens = nltk.word_tokenize(string)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "def placeholder_to_question(sbj_mid_y, questions):\n",
    "    wrong_num_of_ent = []\n",
    "    wrong_num_of_ent_num = []\n",
    "    annotations = []\n",
    "    for i, question in enumerate(questions):\n",
    "        print(i)\n",
    "        question = preprocess_rtrn_string(question)\n",
    "\n",
    "        if len(mid2entity[sbj_mid_y[i]]) != 0:\n",
    "\n",
    "            # find the most probable part of the sentence that matches the true_label of the subject (sbj of the triple)\n",
    "            most_similar_ngram = \"\"\n",
    "            most_similar_ngram_v = -1\n",
    "\n",
    "            n_grams = create_ngrams(question)\n",
    "            for n_gram in n_grams:\n",
    "                similarity = similar(preprocess_rtrn_string(mid2entity[sbj_mid_y[i]][0]), n_gram)\n",
    "                if most_similar_ngram_v < similarity:\n",
    "                    most_similar_ngram = n_gram\n",
    "                    most_similar_ngram_v = similarity\n",
    "\n",
    "            idlbtype = [text_idlabeltype[-1] for text_idlabeltype in mid2dawt[sbj_mid_y[i]]]\n",
    "            sim_ratio = [SequenceMatcher(None, mid2entity[sbj_mid_y[i]][0], text_idlabeltype[-1][1]).ratio()\n",
    "                         for text_idlabeltype in mid2dawt[sbj_mid_y[i]]]\n",
    "            sorted_idlbtype = [x for _, x in sorted(zip(sim_ratio, idlbtype), reverse=True)]\n",
    "            # replacement_ = [\"1_\"]*len(most_similar_ngram.split())\n",
    "            print(sorted_idlbtype[-1])\n",
    "            question_repl = question.replace(most_similar_ngram, sorted_idlbtype[0][-1] + \"_\")\n",
    "            annotations.append(question_repl)\n",
    "        else:\n",
    "            annotations.append(question)\n",
    "\n",
    "    return annotations\n",
    "\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word2vec\n",
    "path_pretrained_emb=\"../data/embeddings/GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format(path_pretrained_emb, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the cells below for training, test, and validation; each time you have to change the corresponding paths (load/save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbj_mid_, rel, question, obj_mid_, samples_with_answer_existing=load_annotated_file(\"../data/SimpleQuestions_v2/annotated_fb_data_train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_correct_samples = []\n",
    "total_wrong_samples = []\n",
    "\n",
    "\n",
    "for ind, id_ in enumerate(samples_with_answer_existing):  # [samples_with_answer_existing[i] for i in x]):\n",
    "    print(\"--------------------------------\", ind, \"---------------------------------------\")\n",
    "\n",
    "    sb = sbj_mid_[id_]\n",
    "    ob = obj_mid_[id_]\n",
    "    \n",
    "    idlbtype = [text_idlabeltype[-1]\n",
    "                for text_idlabeltype in mid2dawt[sb] if text_idlabeltype[-1][-1] != \"MISC\"]\n",
    "\n",
    "    labels_len = [len(id_lb_type[1].split()) for id_lb_type in idlbtype]\n",
    "    longest_labels_ids = [x for _, x in sorted(zip(labels_len, idlbtype), reverse=True)]\n",
    "\n",
    "    pattern = create_replacement(longest_labels_ids)\n",
    "\n",
    "    corpus_new, indx2fb_id = remove_sentences_with_multiple_occurences(mid2dawt[sb], ob)\n",
    "    corpus_new_sentences, indx2fb_id_newsent = split_unpunctuated_text_add_placeholders(corpus_new, indx2fb_id, pattern)\n",
    "\n",
    "    if len(corpus_new_sentences) < 1:\n",
    "        print(\"SKIP\")\n",
    "        continue\n",
    "\n",
    "    rel_txt = preprocess(rel[id_].replace(\"/\", \" \").replace(\"_\", \" \").lower())\n",
    "    weights = get_scores(corpus_new_sentences, rel_txt)\n",
    "    sorted_weight_indx = np.argsort(weights)\n",
    "\n",
    "    cor = True\n",
    "    wrg = True\n",
    "    correct_samples = []\n",
    "    wrong_samples = []\n",
    "\n",
    "    for indx_ in range(1, len(sorted_weight_indx) + 1):\n",
    "        current_indx = sorted_weight_indx[-indx_]\n",
    "        if indx2fb_id_newsent[current_indx][0] == ob and cor:\n",
    "            # print(corpus_new_sentences[current_indx], indx_)\n",
    "            correct_samples.append(corpus_new_sentences[current_indx])\n",
    "            cor = False\n",
    "        elif mid2entity[ob][0] in \" \".join(corpus_new_sentences[current_indx]) and cor:\n",
    "            correct_samples.append(corpus_new_sentences[current_indx])\n",
    "            cor = False\n",
    "        elif indx2fb_id_newsent[current_indx][0] != ob and wrg:\n",
    "            # print(corpus_new_sentences[current_indx], \"@@@@@@@\")\n",
    "            wrong_samples.append(corpus_new_sentences[current_indx])\n",
    "            wrg = False\n",
    "        elif not cor and not wrg:\n",
    "            break\n",
    "\n",
    "    total_correct_samples.append(correct_samples)\n",
    "    total_wrong_samples.append(wrong_samples)\n",
    "\n",
    "# free up some memory \n",
    "del mid2entity\n",
    "del mid2dawt\n",
    "del w2v_model\n",
    "\n",
    "time.sleep(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directory_exists(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "def save_pickle(path, name, python_object):\n",
    "    directory_exists(path)\n",
    "    \n",
    "    with open(path+name,\"wb\") as handle:\n",
    "        pickle.dump(python_object, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pickle = '../data/DAWT/train/'\n",
    "\n",
    "save_pickle(path_pickle, \"total_correct_samples.pickle\", total_correct_samples)\n",
    "\n",
    "save_pickle(path_pickle, \"total_wrong_samples.pickle\", total_wrong_samples)\n",
    "\n",
    "save_pickle(path_pickle, \"samples_with_answer_existing.pickle\", samples_with_answer_existing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (akbc2)",
   "language": "python",
   "name": "akbc2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
