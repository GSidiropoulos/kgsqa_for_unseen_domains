{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import spacy\n",
    "\n",
    "from collections import defaultdict\n",
    "from random import choice\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "#!python3 -m spacy download en_core_web_lg\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(path):\n",
    "    target_predicates = set()\n",
    "\n",
    "    train = pd.read_csv(path + \"annotated_fb_data_train.txt\", sep=\"\\t\", usecols=[1], names=[\"relation\"])\n",
    "    valid = pd.read_csv(path + \"annotated_fb_data_valid.txt\", sep=\"\\t\", usecols=[1], names=[\"relation\"])\n",
    "    test = pd.read_csv(path + \"annotated_fb_data_test.txt\", sep=\"\\t\", usecols=[1], names=[\"relation\"])\n",
    "\n",
    "    train_relations = train[\"relation\"].to_list()\n",
    "    valid_relations = valid[\"relation\"].to_list()\n",
    "    test_relations = test[\"relation\"].to_list()\n",
    "\n",
    "    target_predicates.update(train_relations)\n",
    "    target_predicates.update(valid_relations)\n",
    "    target_predicates.update(test_relations)\n",
    "\n",
    "    predicates = sorted(list(target_predicates))\n",
    "    print(\"Initialize dataset with {} unique predicates.\".format(len(predicates)))\n",
    "    pred_to_ix = {ch: i for i, ch in enumerate(predicates)}\n",
    "    ix_to_pred = {i: ch for i, ch in enumerate(predicates)}\n",
    "\n",
    "    max_len = -1\n",
    "    labels = []\n",
    "    for pred in pred_to_ix.keys():\n",
    "        pred = pred.replace('www.freebase.com/', '').replace('_', ' ').replace('/', ' ')\n",
    "        pred = preprocess(pred)\n",
    "        if max_len < len(pred):\n",
    "            max_len = len(pred)\n",
    "            max_len_ = pred\n",
    "        labels.append(pred)\n",
    "    print('Max length of relation type: ', max_len, max_len_)\n",
    "\n",
    "    return pred_to_ix, ix_to_pred, labels\n",
    "\n",
    "\n",
    "def word_to_id(data, word_to_ix):\n",
    "    \"\"\"Pass a list of lists with tokens, and return list of lists of token ids \"\"\"\n",
    "    return [[word_to_ix[token] if token in word_to_ix else word_to_ix['UNK'] for token in txt] for txt in data]\n",
    "\n",
    "\n",
    "def create_targets(path, pred_to_ix):\n",
    "    \"\"\"Gets the identifier of each relation (appearing as text) in the file provided\"\"\"\n",
    "    df = pd.read_csv(path, sep=\"\\t\", usecols=[1], names=[\"relation\"])\n",
    "    rel = df[\"relation\"].to_list()  # .str.replace(\"www.freebase.com/m/\", \"\")\n",
    "\n",
    "    trgts = []\n",
    "    trgts = [pred_to_ix[p] for p in rel]\n",
    "\n",
    "    return trgts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(samples, keep):\n",
    "    # keep == None if you want to keep them all\n",
    "    sample_tokens = []\n",
    "    for sample in samples:\n",
    "        for sent_tokens in sample[:keep]:\n",
    "            sample_tokens.extend(sent_tokens)\n",
    "\n",
    "    return sample_tokens\n",
    "\n",
    "\n",
    "def keep_less_samples(samples, keep):\n",
    "    samples_new = []\n",
    "    num_of_sent = []\n",
    "\n",
    "    for sample in samples:\n",
    "        keep_samples = sample[:keep]\n",
    "        samples_new.append(keep_samples)\n",
    "        num_of_sent.append(len(keep_samples))\n",
    "\n",
    "    return samples_new, num_of_sent\n",
    "\n",
    "\n",
    "def keep_less_negative_samples(samples, keep):\n",
    "    samples_new = []\n",
    "    for i, sample in enumerate(samples):\n",
    "        keep_samples = sample[:keep[i]]\n",
    "        samples_new.append(keep_samples)\n",
    "\n",
    "    return samples_new\n",
    "\n",
    "\n",
    "def preprocess(string):\n",
    "    tokens = nltk.word_tokenize(string)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def words_to_ids(samples, word_to_ix, questions, relations, labels):\n",
    "    samples_flat = []\n",
    "    questions_ = []\n",
    "    relations_ = []\n",
    "    labels_ = []\n",
    "    for indx, sentences in enumerate(samples):\n",
    "        samples_flat.extend(sentences)\n",
    "        questions_.extend([questions[indx]] * len(sentences))\n",
    "        relations_.extend([relations[indx]] * len(sentences))\n",
    "        labels_.extend([labels[indx]] * len(sentences))\n",
    "\n",
    "    data = []\n",
    "    for sentence in samples_flat:\n",
    "        # data.append([word_to_ix[token] for token in sentence])\n",
    "        data.append([word_to_ix[token] if token in word_to_ix else word_to_ix['UNK'] for token in sentence])\n",
    "    return data, questions_, relations_, labels_\n",
    "\n",
    "\n",
    "def suffle_lists(a, b):\n",
    "    combined = list(zip(a, b))\n",
    "    random.shuffle(combined)\n",
    "\n",
    "    a[:], b[:] = zip(*combined)\n",
    "\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict(questions_string, correct_samples, wrong_samples):\n",
    "    tokens = [token.lower() for sentence in questions_string for token in preprocess(sentence)]\n",
    "    # tokens = [token.lower() for sentence in questions_string for token in sentence.split()]\n",
    "    tokens_correct_samples = get_tokens(correct_samples, None)\n",
    "    tokens_wrong_samples = get_tokens(wrong_samples, None)\n",
    "    tokens.extend(tokens_correct_samples)\n",
    "    tokens.extend(tokens_wrong_samples)\n",
    "\n",
    "    words = sorted(list(set(tokens)))\n",
    "    data_size, vocab_size = len(tokens), len(words)\n",
    "    print(\"Initialize dataset with {} characters, {} unique.\".format(data_size, vocab_size))\n",
    "    word_to_ix = {ch: i + 1 for i, ch in enumerate(words)}\n",
    "    ix_to_word = {i + 1: ch for i, ch in enumerate(words)}\n",
    "\n",
    "    word_to_ix['UNK'] = len(word_to_ix)\n",
    "    ix_to_word[len(ix_to_word)] = 'UNK'\n",
    "\n",
    "    return word_to_ix, ix_to_word\n",
    "\n",
    "\n",
    "def create_data(path, samples_with_answer_existing, correct_samples, wrong_samples, word_to_ix, realtions, labels,\n",
    "                pred_txt):\n",
    "    sbj_mid_ = list()\n",
    "    obj_mid_ = list()\n",
    "    question = list()\n",
    "    with open(path) as f:\n",
    "        for line in f.readlines():\n",
    "            s = line.split('\\t')[0]\n",
    "            s = s.replace(\"www.freebase.com/m/\", \"\")\n",
    "            sbj_mid_.append(s)\n",
    "\n",
    "            s = line.split('\\t')[2]\n",
    "            s = s.replace(\"www.freebase.com/m/\", \"\")\n",
    "            obj_mid_.append(s)\n",
    "\n",
    "            question.append(line.split('\\t')[3])\n",
    "    \n",
    "    questions_ex_answ = [preprocess(question[q]) for q in samples_with_answer_existing]\n",
    "    questions_ex_answ_ = []\n",
    "    for q in questions_ex_answ:\n",
    "        sentence_ids = [word_to_ix[token.lower()] if token.lower() in word_to_ix else word_to_ix['UNK'] for token in q]\n",
    "        questions_ex_answ_.append(sentence_ids)\n",
    "    \n",
    "    negative_relations = list()\n",
    "    negative_labels = list()\n",
    "    for inp in labels:\n",
    "        neg = choice(labels_intersect[inp])\n",
    "        negative_relations.append(pred_txt[neg])\n",
    "        negative_labels.append(neg)\n",
    "\n",
    "    # create question and text data\n",
    "    correct_samples_less, num = keep_less_samples(correct_samples, 3)\n",
    "    data_correct, questions_corr_ans, relations_corr_ans, labels_corr_ans = words_to_ids(correct_samples_less,\n",
    "                                                                                         word_to_ix, questions_ex_answ_,\n",
    "                                                                                         realtions, labels)\n",
    "\n",
    "    wrong_samples_less = keep_less_negative_samples(wrong_samples, num)\n",
    "    data_wrong, questions_wrong_ans, relations_wrong_ans, labels_wrong_ans = words_to_ids(correct_samples_less,\n",
    "                                                                                          word_to_ix,\n",
    "                                                                                          questions_ex_answ_,\n",
    "                                                                                          negative_relations,\n",
    "                                                                                          negative_labels)\n",
    "\n",
    "    data_correct = keras.preprocessing.sequence.pad_sequences(data_correct, maxlen=100, dtype='int32', padding='post',\n",
    "                                                              truncating='post', value=0.0)\n",
    "    data_wrong = keras.preprocessing.sequence.pad_sequences(data_wrong, maxlen=100, dtype='int32', padding='post',\n",
    "                                                            truncating='post', value=0.0)\n",
    "\n",
    "    relations_corr_ans = keras.preprocessing.sequence.pad_sequences(relations_corr_ans, maxlen=17, dtype='int32',\n",
    "                                                                    padding='post', truncating='post', value=0.0)\n",
    "    relations_wrong_ans = keras.preprocessing.sequence.pad_sequences(relations_wrong_ans, maxlen=17, dtype='int32',\n",
    "                                                                     padding='post', truncating='post', value=0.0)\n",
    "\n",
    "    questions_corr_ans = keras.preprocessing.sequence.pad_sequences(questions_corr_ans, maxlen=36, dtype='int32',\n",
    "                                                                    padding='post', truncating='post', value=0.0)\n",
    "    questions_wrong_ans = keras.preprocessing.sequence.pad_sequences(questions_wrong_ans, maxlen=36, dtype='int32',\n",
    "                                                                     padding='post', truncating='post', value=0.0)\n",
    "\n",
    "    print('Data correct: ', data_correct.shape)\n",
    "    print('Data wrong: ', data_wrong.shape)\n",
    "    \n",
    "    data = np.concatenate((data_correct, data_wrong), axis=0)\n",
    "    data_questions = np.concatenate((questions_corr_ans, questions_wrong_ans), axis=0)\n",
    "    data_relations = np.concatenate((relations_corr_ans, relations_wrong_ans), axis=0)\n",
    "    data_labels = np.concatenate((labels_corr_ans, labels_wrong_ans), axis=0)\n",
    "\n",
    "    # create targets\n",
    "    targets = []\n",
    "    for i in range(len(data_correct)):\n",
    "        targets.append(np.array([0, 1]))\n",
    "\n",
    "    for i in range(len(data_wrong)):\n",
    "        targets.append(np.array([1, 0]))\n",
    "\n",
    "    targets = np.array(targets)\n",
    "\n",
    "    return data, data_questions, targets, data_relations, data_labels\n",
    "\n",
    "\n",
    "def get_spo_question(annotated_fb_data_path=\"\", file_name=\"\"):\n",
    "    \"\"\"\n",
    "    :param annotated_fb_data_path: path leading to the original SimpleQuestions dataset\n",
    "    :param file_name: name of the data file e.g annotated_fb_data_test.txt\n",
    "    :return: 4 lists -> subject, predicate, object and question(as text)\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(annotated_fb_data_path + file_name, sep=\"\\t\", usecols=[0, 1, 2, 3],\n",
    "                     names=[\"sbj\", \"relation\", \"obj\", \"question\"])\n",
    "    sbj_mid = df[\"sbj\"].str.replace(\"www.freebase.com/m/\", \"\").to_list()\n",
    "    predicate = df[\"relation\"].str.replace(\"www.freebase.com/\", \"\").to_list()\n",
    "    obj_mid = df[\"obj\"].str.replace(\"www.freebase.com/m/\", \"\").to_list()\n",
    "    questions = df[\"question\"].to_list()\n",
    "\n",
    "    return sbj_mid, predicate, obj_mid, questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pickle = '../data/DAWT/train/'\n",
    "with open(path_pickle + 'total_correct_samples' + '.pickle', 'rb') as f:\n",
    "    correct_samples_train = pickle.load(f)\n",
    "    \n",
    "with open(path_pickle + 'total_wrong_samples' + '.pickle', 'rb') as f:\n",
    "    wrong_samples_train = pickle.load(f)\n",
    "    \n",
    "with open(path_pickle + 'samples_with_answer_existing' + '.pickle', 'rb') as f:\n",
    "    samples_with_answer_existing_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sbj, _train_rel, train_obj, question_train = get_spo_question('../data/SimpleQuestions_v2/annotated_fb_data_train.txt')\n",
    "\n",
    "# word to index, index to word\n",
    "word2ix, ix2word = create_dict(question_train, correct_samples_train, wrong_samples_train)\n",
    "\n",
    "# relation to index, index to relation\n",
    "pred2ix, ix2pred, label_text = load_labels('../data/SimpleQuestions_v2/')\n",
    "\n",
    "# relation text to list of index \n",
    "relation = word_to_id(label_text, word2ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### load train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = create_targets('../data/SimpleQuestions_v2/annotated_fb_data_train.txt',pred2ix)\n",
    "labels_train_sub = [labels_train[s] for s in samples_with_answer_existing_train]\n",
    "relation_train_sub = [relation[labels_train[s]] for s in samples_with_answer_existing_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### load validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pickle = '../data/DAWT/valid/'\n",
    "\n",
    "with open(path_pickle + 'total_correct_samples' + '.pickle', 'rb') as f:\n",
    "    correct_samples_valid = pickle.load(f)\n",
    "    \n",
    "with open(path_pickle + 'total_wrong_samples' + '.pickle', 'rb') as f:\n",
    "    wrong_samples_valid = pickle.load(f)\n",
    "    \n",
    "with open(path_pickle + 'samples_with_answer_existing' + '.pickle', 'rb') as f:\n",
    "    samples_with_answer_existing_valid = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_valid = create_targets('../data/SimpleQuestions_v2/annotated_fb_data_valid.txt',pred2ix)\n",
    "\n",
    "labels_valid_sub = [labels_valid[s] for s in samples_with_answer_existing_valid]\n",
    "relation_valid_sub = [relation[labels_valid[s]] for s in samples_with_answer_existing_valid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### load test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pickle = '../data/DAWT/test/'\n",
    "\n",
    "with open(path_pickle + 'total_correct_samples' + '.pickle', 'rb') as f:\n",
    "    correct_samples_test = pickle.load(f)\n",
    "    \n",
    "with open(path_pickle + 'total_wrong_samples' + '.pickle', 'rb') as f:\n",
    "    wrong_samples_test = pickle.load(f)\n",
    "    \n",
    "with open(path_pickle + 'samples_with_answer_existing' + '.pickle', 'rb') as f:\n",
    "    samples_with_answer_existing_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test = create_targets('../data/SimpleQuestions_v2/annotated_fb_data_test.txt',pred2ix)\n",
    "\n",
    "labels_test_sub = [labels_test[s] for s in samples_with_answer_existing_test]\n",
    "relation_test_sub = [relation[labels_test[s]] for s in samples_with_answer_existing_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the original vocabs zero shot QG work of Elsahar @NAACL2018\n",
    "# we need to make mappings between ours and the original\n",
    "\n",
    "classes=[]\n",
    "with open('../data/zero_qg/property.vocab','r') as handle:\n",
    "    rel = handle.readlines()\n",
    "\n",
    "for r in rel:\n",
    "    classes.append(r.replace(\"\\n\",\"\"))\n",
    "    \n",
    "\n",
    "with open('../data/zero_qg/word.vocab','r') as f:\n",
    "    vocab = f.readlines()\n",
    "vocab_original = []\n",
    "for i in vocab:\n",
    "    vocab_original.append(i.replace(\"\\n\",\"\"))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicate2sentences = dict()\n",
    "# init\n",
    "for i in (set(labels_train_sub).union(set(labels_valid_sub))).union(set(labels_test_sub)):\n",
    "    predicate2sentences[i]=''\n",
    "    \n",
    "for i,s in enumerate(correct_samples_train):\n",
    "    if len(s) !=0:\n",
    "        predicate_id = labels_train_sub[i]\n",
    "        predicate2sentences[predicate_id] = predicate2sentences[predicate_id]+' '.join(s[0])+'. '\n",
    "        \n",
    "\n",
    "for i,s in enumerate(correct_samples_valid):\n",
    "    if len(s) !=0:\n",
    "        predicate_id = labels_valid_sub[i]\n",
    "        predicate2sentences[predicate_id] = predicate2sentences[predicate_id]+' '.join(s[0])+'. '\n",
    "        \n",
    "for i,s in enumerate(correct_samples_test):\n",
    "    if len(s) !=0:\n",
    "        predicate_id = labels_test_sub[i]\n",
    "        predicate2sentences[predicate_id] = predicate2sentences[predicate_id]+' '.join(s[0])+'. '\n",
    "        \n",
    "        \n",
    "corpus = []\n",
    "preds = []\n",
    "for k,v in predicate2sentences.items():\n",
    "    corpus.append(v)\n",
    "    preds.append(k)\n",
    "\n",
    "# update the stopwords set\n",
    "l=['organization_','location_','misc_','person_','film_','book_','event_','category','s']\n",
    "listoffrozensets = [frozenset(l),text.ENGLISH_STOP_WORDS]\n",
    "stop_w = frozenset().union(*listoffrozensets)\n",
    "    \n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,1),stop_words=stop_w,token_pattern='(?u)\\\\b\\\\w*[a-zA-Z]\\\\w*\\\\b')\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "features = vectorizer.get_feature_names()\n",
    "\n",
    "#print(vectorizer.get_feature_names())\n",
    "print(X.shape)\n",
    "id2key = dict()\n",
    "X_array = X.toarray()\n",
    "top_k = 10\n",
    "predicate2keywords = dict()\n",
    "for doc in range(X.shape[0]):\n",
    "    doc_sorted = np.argsort(X_array[doc])[::-1]\n",
    "    print(preds[doc], ix2pred[preds[doc]],classes.index(ix2pred[preds[doc]]))\n",
    "    keywords = [features[f] for f in doc_sorted[:top_k]]\n",
    "    print(keywords)\n",
    "    print()\n",
    "    \n",
    "    predicate2keywords[preds[doc]] =  keywords\n",
    "    id2key[classes.index(ix2pred[preds[doc]])]=keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2keypos = defaultdict(list)\n",
    "for k,v in id2key.items():\n",
    "    c=1\n",
    "    for i in v:\n",
    "        doc = nlp(i)\n",
    "        for token in doc:\n",
    "            pos = token.pos_\n",
    "            pos = '_DEP_'+pos+'_'\n",
    "            if pos in [p[1] for p in id2keypos[k]]: \n",
    "                pos = pos +str(c)+'_'\n",
    "                c +=1\n",
    "            id2keypos[k].append((str(token), str(pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_ = set()\n",
    "for k,v in id2keypos.items():\n",
    "    for token_,pos_tag_ in v:\n",
    "        \n",
    "        if str(token_) not in vocab_original:\n",
    "            vocab_.add(str(token_))\n",
    "        if str(pos_tag_) not in vocab_original:            \n",
    "            vocab_.add(str(pos_tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vocab_original))\n",
    "vocab_original.extend(vocab_)\n",
    "print(len(vocab_original))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directory_exists(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "def save_pickle(path, name, python_object):\n",
    "    directory_exists(path)\n",
    "    \n",
    "    with open(path+name,\"wb\") as handle:\n",
    "        pickle.dump(python_object, handle)\n",
    "        \n",
    "def save_txt(path, name, python_object):\n",
    "    directory_exists(path)\n",
    "    \n",
    "    with open(path+name,\"w\") as handle:\n",
    "        for item in python_object:\n",
    "            handle.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(\"../data/DAWT/keywords/\", \"pred2key.pkl\", predicate2keywords)\n",
    "save_pickle(\"../data/DAWT/keywords/\", \"id2keypos.pkl\", id2keypos)\n",
    "save_txt(\"../data/DAWT/keywords/\", \"word.vocab\", vocab_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (akbc2)",
   "language": "python",
   "name": "akbc2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
