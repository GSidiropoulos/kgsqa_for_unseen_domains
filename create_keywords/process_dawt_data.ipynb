{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import gzip \n",
    "import time\n",
    "import gc\n",
    "import math\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import spacy\n",
    "#!python3 -m spacy download en_core_web_lg\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dictionaries wiki2mid, mid2entity and entity2mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikipedia url -> MID\n",
    "wiki_url2fb_id = dict()\n",
    "with open('../data/freebase_id_wiki_url.en_fixed') as f:\n",
    "        for line in f.readlines():\n",
    "            fb_id = line.split()[0]\n",
    "            wiki_url = line.split()[1]\n",
    "            wiki_url2fb_id [wiki_url] = fb_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contains only cases within SimpleQuestions (was an assumption of the paper)\n",
    "# however more general mid2entity (for all the FB) could be used \n",
    "# and achieve better results\n",
    "\n",
    "with open('../data/' + 'mid2entity_list_simple' + '.pkl', 'rb') as f:\n",
    "    mid2entity = pickle.load(f)\n",
    "    \n",
    "with open('../data/' + 'entity2mid_list_simple' + '.pkl', 'rb') as f:\n",
    "    entity2mid = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniquify_list(l):\n",
    "    set_of_tuples = set(tuple(row) for row in l)\n",
    "    list_of_list = [list(item) for item in set(tuple(row) for row in set_of_tuples)]\n",
    "    return list_of_list\n",
    "\n",
    "def get_positions(df_entities, fb_id):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for ent in df_entities:\n",
    "        if fb_id == ent['id_str']:\n",
    "            start_positions.append(ent['start_position'])\n",
    "            end_positions.append(ent['end_position'])\n",
    "            \n",
    "    return start_positions, end_positions\n",
    "\n",
    "def get_positions(df_entities):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    mids = []\n",
    "    for ent in df_entities:\n",
    "        if ent['id_str'] in mid2entity:\n",
    "            start_positions.append(ent['start_position'])\n",
    "            end_positions.append(ent['end_position'])\n",
    "            mids.append([ent['id_str'], ent['raw_form'],ent['type']])\n",
    "            \n",
    "            \n",
    "    return start_positions, end_positions, mids\n",
    "\n",
    "def get_positions_text(df_tokens, label):\n",
    "    # when the fb_id in the json is none\n",
    "    # label is the wiki title en.wikipedia.org/wiki/Željko_Obradović -> Željko_Obradović\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    pos =[]\n",
    "    sentences = [tok['raw_form'].lower() for tok in df_tokens]\n",
    "    wiki_label = [i.lower() for i in label.split('_') if i not in stopWords]\n",
    "    for word in wiki_label:\n",
    "        pos.append( [p for p,v in enumerate(sentences) if word ==v] )\n",
    "    pos_flatten = [pos_ for pos_list in pos for pos_ in pos_list]     \n",
    "    return pos_flatten, pos_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentence(df_tokens, start_pos, end_pos):\n",
    "    \n",
    "    cont = True\n",
    "    pos = start_pos\n",
    "    sentence_backward = []\n",
    "\n",
    "    #\n",
    "    counter = 0 # because it is possible that the word we want appears at the end of the sentence so the first if wouldnt work\n",
    "    while cont:\n",
    "        if 'section_break' in df_tokens[pos].keys() and counter != 0:\n",
    "            cont = False\n",
    "        else:\n",
    "            sentence_backward.append(df_tokens[pos]['raw_form'])\n",
    "            if pos == 0:\n",
    "                cont = False\n",
    "            else:\n",
    "                pos -= 1\n",
    "        counter += 1\n",
    "    sentence_backward.reverse()\n",
    "    \n",
    "    #\n",
    "    cont = True\n",
    "    pos = end_pos\n",
    "    sentence_forward = []\n",
    "\n",
    "    while (cont and pos< len(df_tokens) ):\n",
    "        if 'section_break' in df_tokens[pos].keys():\n",
    "            sentence_forward.append(df_tokens[pos]['raw_form'])\n",
    "            cont = False\n",
    "        else:\n",
    "            sentence_forward.append(df_tokens[pos]['raw_form'])\n",
    "            pos += 1\n",
    "\n",
    "    \n",
    "    if start_pos == end_pos:\n",
    "        sentence_forward = sentence_forward[1:]\n",
    "    elif (end_pos-start_pos) > 1 :\n",
    "        words_in_between = [df_tokens[i]['raw_form'] for i in range(start_pos+1, end_pos)]\n",
    "        sentence_forward = words_in_between + sentence_forward\n",
    "        \n",
    "    \n",
    "    sentence =  sentence_backward + sentence_forward   \n",
    "\n",
    "    return sentence\n",
    "\n",
    "def find_sentences (df_tokens, start_positions, end_positions, mids):\n",
    "    sentences = []\n",
    "    occurrences_num = len(start_positions) \n",
    "\n",
    "    for oc in range(occurrences_num):\n",
    "        start_pos = start_positions[oc]\n",
    "        end_pos = end_positions[oc]\n",
    "        \n",
    "        sentences.append(find_sentence(df_tokens, start_pos, end_pos)+[mids[oc]])\n",
    "    \n",
    "    # uniquify list\n",
    "    #sentences = uniquify_list(sentences)\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_id2sentences = dict()\n",
    "fb_id2sentences_wikiurl =dict()\n",
    "fb_id2sentences_ent2mid =dict()\n",
    "\n",
    "#mids = list(mid2entity.keys())\n",
    "path_pickle = '../data/DAWT/part00/'\n",
    "for i in range (1,41):\n",
    "    input_file = '../data/DAWT/part00/split_{}'.format(i)\n",
    "    df = pd.read_json(input_file, lines=True)\n",
    "    fb_ids = [w_id.split(':')[3] for w_id in df['id']]\n",
    "    label = [w_id.split(':')[4] for w_id in df['id']]\n",
    "    print('---------------------------------------------',i,'--------------------------------------------------')\n",
    "    \n",
    "    for pos, fb_id in enumerate(fb_ids):\n",
    "        if fb_id in mid2entity:\n",
    "            df_entities = df['entities'][pos]\n",
    "            df_tokens = df['tokens'][pos]\n",
    "\n",
    "            if isinstance(df_entities, list):\n",
    "                start_positions, end_positions, mids = get_positions(df_entities)#get_positions(df_entities, fb_id)\n",
    "                sentences  = find_sentences (df_tokens, start_positions, end_positions, mids)\n",
    "            else:\n",
    "                #print('nan')\n",
    "                sentences = []\n",
    "\n",
    "            # add to dictionary\n",
    "            fb_id2sentences[fb_id] = sentences\n",
    "            \n",
    "        elif 'https://en.wikipedia.org/wiki/'+label[pos] in wiki_url2fb_id:\n",
    "            fb_id = wiki_url2fb_id['https://en.wikipedia.org/wiki/'+label[pos]]\n",
    "            \n",
    "            if fb_id in mid2entity:\n",
    "                df_entities = df['entities'][pos]\n",
    "                df_tokens = df['tokens'][pos]\n",
    "\n",
    "                if isinstance(df_entities, list):\n",
    "                    start_positions, end_positions, mids = get_positions(df_entities)#get_positions(df_entities, fb_id)\n",
    "                    sentences  = find_sentences (df_tokens, start_positions, end_positions, mids)\n",
    "\n",
    "                    if len(sentences)<1 :\n",
    "                        start_positions, end_positions, mids = get_positions(df_entities)#get_positions_text(df_tokens, label[pos])\n",
    "                        sentences  = find_sentences (df_tokens, start_positions, end_positions, mids)\n",
    "\n",
    "                else:\n",
    "                    sentences = []\n",
    "\n",
    "                # add to dictionary\n",
    "                fb_id2sentences_wikiurl[fb_id] = sentences\n",
    "                \n",
    "        elif ' '.join(label[pos].lower().split('_')) in entity2mid:\n",
    "            fb_id = entity2mid[' '.join(label[pos].lower().split('_'))]\n",
    "            \n",
    "            df_entities = df['entities'][pos]\n",
    "            df_tokens = df['tokens'][pos]\n",
    "\n",
    "            if isinstance(df_entities, list):\n",
    "                start_positions, end_positions, mids = get_positions(df_entities)#get_positions_text(df_tokens, label[pos])\n",
    "                sentences  = find_sentences (df_tokens, start_positions, end_positions, mids)\n",
    "            else:\n",
    "                sentences = []\n",
    "\n",
    "            # add to dictionary\n",
    "            for mid in fb_id: \n",
    "                fb_id2sentences_ent2mid[mid] = sentences\n",
    "    df = None\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dictionary to pickle file\n",
    "with open(path_pickle+'fb_id2sentences_new.pickle', 'wb') as handle:\n",
    "    pickle.dump(fb_id2sentences, handle)\n",
    "\n",
    "with open(path_pickle+'fb_id2sentences_wikiurl.pickle', 'wb') as handle:\n",
    "    pickle.dump(fb_id2sentences_wikiurl, handle)\n",
    "\n",
    "with open(path_pickle+'fb_id2sentences_ent2mid.pickle', 'wb') as handle:\n",
    "    pickle.dump(fb_id2sentences_ent2mid, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note:\n",
    "To create the final fb_id2sentences_idstr_label_type just combine the 3 fb_id2sentences_ent2mid into a single dictionary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (akbc2)",
   "language": "python",
   "name": "akbc2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
