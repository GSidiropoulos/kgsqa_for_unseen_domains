{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freebase 2M subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/SimpleQuestions_v2/freebase-subsets/freebase-FB2M.txt'\n",
    "\n",
    "start = time.time()\n",
    "df = pd.read_table(data_path, sep=\"\\t\", header=None, names=[\"subject\", \"relation\", \"object\"])\n",
    "finish = time.time()\n",
    "print(\"time taken: {}s\".format(finish-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of unique queries that can be answered: ', len(set(df['subject']+df['object']+df['relation'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import defaultdict\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "_tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngrams(text):\n",
    "\n",
    "    n_grams = list() \n",
    "    for i in range(1, len(text)+1):\n",
    "        n_gram = ngrams(text,i)\n",
    "\n",
    "        for gram in n_gram:\n",
    "            n_grams.append(' '.join(gram))\n",
    "\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dictionary of mid -> list of entity names sharing the mid\n",
    "with open('../data/mid2ent.pkl','rb') as file:\n",
    "    mid2ent = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inverted Index of entity name to from ngram of entity name\n",
    "# to entities sharing this ngram to their names or exact match\n",
    "\n",
    "inv_index = defaultdict(list)\n",
    "i=0\n",
    "for k,v in mid2ent.items():\n",
    "    ngr  = create_ngrams(_tokenizer.tokenize(v[0]))\n",
    "\n",
    "    for gram in ngr:\n",
    "        vectorizer = TfidfVectorizer(ngram_range=(len(gram.split()),len(gram.split())),token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "        X = vectorizer.fit_transform(v)\n",
    "        features = vectorizer.get_feature_names()\n",
    "        X_array = X.toarray()\n",
    "        score =X_array[0][features.index(gram)]\n",
    "        inv_index[gram].append((k,v,score))\n",
    "        \n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/inverted_index.pkl','wb') as handle:\n",
    "    pickle.dump(inv_index,handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
